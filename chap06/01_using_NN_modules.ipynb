{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really good explainer on how Neural Networks approximate functions using only linear functions and activation functions is Michael Nielsen's *A visual proof that neural nets can compute any function*. See [this link.](https://neuralnetworksanddeeplearning.com/chap4.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAKE SURE TO ADD NOTES ON ACTIVATION FUNCTIONS ONCE YOU HAVE A BETTER GRASP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch's NN modules\n",
    "In our previous linear model (chap 05), we wrote a simple linear function to map a line through our data points. Below is the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where w is the 'weight' and b is the 'bias'\n",
    "def model(unknown_unit, w, b):\n",
    "    return w * unknown_unit + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides *modules* (known as *layers* in other frameworks) in the sub-library `torch.nn`. A PyTorch module is a class deriving from the `torch.nn.Module` base class. A module can have one or more `Parameter` instances as attributes which are the tensors to be optimized during training (in our case *w* and *b*). A module can also have one or more *submodules* which are subclasses of `nn.Module` as attributes and will be able to track their parameters as well. <br><br>\n",
    "If we wanted to replace our above model function, we could use the subclass of `nn.Module`, `nn.Linear`. `nn.Linear` has the following parameters:\n",
    "* in_features(int) - the size of the input.\n",
    "* out_features(int) - the size of the output\n",
    "* bias(bool) - defaults to `True`. If set to `False`, the model will not learn a bias. No *b* parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A Little tangent on matrix-vector products to help me full understand the above parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # import torch.nn with a convenient alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0369, -0.6950],\n",
       "        [-0.6591,  0.6533],\n",
       "        [ 0.4035,  0.4890]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(2,3,False) # input 2D vector, output 3D vector, NO BIAS\n",
    "linear_model.weight # weights are a 3 X 2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3530,  0.6475,  1.3814], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.Tensor([1.0, 2.0]) # the input 2D vector\n",
    "\n",
    "# since our model is a linear function with no bias, we are essentially doing a matrix-vector multiplication\n",
    "# between our input 2D vector and a 3X2 matrix. This means, we will get the prescribed 3D vector as output!\n",
    "linear_model.weight.matmul(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Back to the book... <br><br>\n",
    "In chapter 5, we defined our model as the linear function above. We passed in a tensor with `requires_grad=True`, for *w* and *b*. In doing so, Pytorch is able to solve the derivative of the loss function in respect to *w* and *b* through all the steps those two parameters travelled, including our model.<br><br>\n",
    "Below is how we set this up previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining our parameters w and b with autograd enabled\n",
    "params = torch.tensor([0.0, 1.0], requires_grad=True)\n",
    "\n",
    "# defining our model\n",
    "def model(unknown_unit, w, b):\n",
    "    return w * unknown_unit + b\n",
    "\n",
    "# placeholder unknown_unit\n",
    "t_u = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# we can then call this in our training loop like so:\n",
    "model(t_u, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the *autograd enabled* parameters and the model with the submodule `nn.Linear` described above. If we want to view the *weight* we can used the property `.weight`, and to get the *bias* we can use the property `.bias`. Furthermore, if we need to pass the parameters to the optimizer, we can call the method `.parameters()`<br><br>\n",
    "To replace the `params` tensor and `model` function above, we can simply write this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.8310]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5477], requires_grad=True),\n",
       " <generator object Module.parameters at 0x7fb01eb035a0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1) #bias defaults to true\n",
    "\n",
    "linear_model.weight, linear_model.bias, linear_model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a forward pass, we no longer pass the parameters and data into the model; we only pass the the data by calling the model. For instance, to pass `t_u` as above, we would write `model_output = linear_model(t_u)`. We can try to pass our placeholder *unknown unit tensor* `t_u` into `linear_model` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 1x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_u\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/pytorch_learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/pytorch_learning/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/pytorch_learning/env/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 1x1)"
     ]
    }
   ],
   "source": [
    "model_output = linear_model(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "We can see above that our data input `t_u` is the wrong shape. We can't multiply the 1x3 vector `t_u` by the 1x1 matrix that is the *weight* parameter in our linear model. <br><br>\n",
    "We need to *batch* our input data to create tensor of size $B\\times Nin$ where $B$ is the size of the batch, and $Nin$ is the size of the input. Since we have an input size of 1 in our `linear_model` we need a tensor of size 3X1 for our placeholder data `t_u`. We can easily do this using `unsqueeze` which will add an extra dimension at axis 1 of our input.<br><br>\n",
    "Batching also allows our GPU to compute the inputs in paralell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batching our input data\n",
    "t_u = t_u.clone().unsqueeze(1)\n",
    "t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing this data into our linear model will now work!\n",
    "model_output = linear_model(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing the Loss Function\n",
    "PyTorch also provides a bunch of pre-coded loss functions. Previously, we coded our own *mean squared loss* function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our loss function - mean squared loss\n",
    "def loss_fn (temps_predicted, temps_actual):\n",
    "    squared_diff = (temps_predicted - temps_actual) ** 2\n",
    "    return squared_diff.mean() # taking the mean of the squared dif vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply replace this with `nn.MSELoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The revised training loop.\n",
    "To keep things module, we will now pass in our optimizer, model, and loss function as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        t_p_train = model(t_u_train) # forward pass on training data\n",
    "        loss_train = loss_fn(t_p_train, t_c_train) # evaluating the loss\n",
    "\n",
    "        with torch.no_grad(): # we don't need a graph for validation - switch it off!\n",
    "            t_p_val = model(t_u_val) # evaluating the validation data\n",
    "            loss_val = loss_fn(t_p_val, t_c_val) # calculating loss for validation set\n",
    "\n",
    "        optimizer.zero_grad() # zeroing the gradient\n",
    "        loss_train.backward() # backward pass on loss_train\n",
    "        optimizer.step() # update the parameters\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training Loss {loss_train.item():.4f}, Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring our training data back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.0000],\n",
       "         [13.0000],\n",
       "         [ 3.0000],\n",
       "         [11.0000],\n",
       "         [15.0000],\n",
       "         [-4.0000],\n",
       "         [ 0.5000],\n",
       "         [21.0000],\n",
       "         [28.0000]]),\n",
       " tensor([[4.8400],\n",
       "         [6.0400],\n",
       "         [3.3900],\n",
       "         [5.6300],\n",
       "         [5.8200],\n",
       "         [2.1800],\n",
       "         [3.5700],\n",
       "         [6.8400],\n",
       "         [8.1900]]),\n",
       " tensor([[ 8.],\n",
       "         [14.]]),\n",
       " tensor([[4.8900],\n",
       "         [5.5900]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just copying our split data from the last chapter - t_u already normalized\n",
    "t_c_train = torch.tensor([6.0, 13.0, 3.0, 11.0, 15.0, -4.0, 0.5, 21.0, 28.0])\n",
    "t_u_train = torch.tensor([4.84, 6.04, 3.39, 5.63, 5.82, 2.18, 3.57, 6.84, 8.19])\n",
    "\n",
    "t_c_val = torch.tensor([8.0, 14.0])\n",
    "t_u_val = torch.tensor([4.89, 5.59])\n",
    "\n",
    "# batching the data\n",
    "t_c_train = t_c_train.unsqueeze(1)\n",
    "t_u_train = t_u_train.unsqueeze(1)\n",
    "t_c_val = t_c_val.unsqueeze(1)\n",
    "t_u_val = t_u_val.unsqueeze(1)\n",
    "\n",
    "t_c_train, t_u_train, t_c_val, t_u_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our model, optimizer, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1, 1) # our linear model - 1X1 tensor in and out, bias = true\n",
    "\n",
    "# passing our linear model's parameters into our optimizer and setting the learning rate\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-2) \n",
    "\n",
    "# defining our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 132.1691, Validation loss 74.8418\n",
      "Epoch 1000, Training Loss 3.8270, Validation loss 1.6122\n",
      "Epoch 2000, Training Loss 3.2969, Validation loss 1.3587\n",
      "Epoch 3000, Training Loss 3.2878, Validation loss 1.3322\n"
     ]
    }
   ],
   "source": [
    "# running the training loop\n",
    "training_loop(\n",
    "    num_epochs= 3000,\n",
    "    optimizer= optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=loss_function,\n",
    "    t_u_train=t_u_train,\n",
    "    t_u_val=t_u_val,\n",
    "    t_c_train=t_c_train,\n",
    "    t_c_val=t_c_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
